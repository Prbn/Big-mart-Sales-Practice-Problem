{
    "collab_server" : "",
    "contents" : "\n\n##################\n### REGRESSION ###\n##################\n\n# Simple linear Regression #\n#==========================#\n\n# Importing Data\n#---------------\n\n# Setting up new working directory using the\nsetwd('D:\\\\Work\\\\ML\\\\Super Data Science\\\\Machine Learning A-Z Template Folder\\\\Part 2 - Regression')\n\n# Putting the data frame into an object called stats\nSalary_d <- read.csv('Section 4 - Simple Linear Regression\\\\Salary_Data.csv')\n\n# Exploring data\n# --------------\n\n# Summary of dataset\nsummary(Salary_d)\n# Structure of dataset\nstr(Salary_d)\n\n# Splitting the data set\n# ----------------------\n\n# Splitting the dataset into the training set and Test set\n# Using the package caTools\nlibrary(caTools)\nset.seed(123)\nsplit <- sample.split(Salary_d$Salary, SplitRatio = 2/3)\ntraining_set <- subset(Salary_d, split == TRUE)\ntest_set <- subset(Salary_d, split == FALSE)\n\n# Feature Scaling\n# ---------------\n\n# training_set[,] <- scale(training_set[,])\n# test_set[,] <- scale(test_set[,])\n# As the library used for linear regression already has featuring scaling built in, it does not requir feature scaling\n\n# Simple linear model\n# -------------------\n\n# Fitting Simple Linear Regression to the training set.\n# Using the lm() function\n# creating regressor variable to store the linearmodel\nregressor = lm(formula = Salary ~ YearsExperience, data = training_set)\n# The formula is dependent variable expressed as a linear combination of the independent variable\n# The data on which the to train needs to be specified.\n# Info about the regressor using summary() function\nsummary(regressor)\n\n# Predicting the Test set results\n# Using the predict() function\n# storing the prediction into a functon\ny_pred <- predict(regressor,newdata = test_set)\n# Requirs the liner model to use as a basis to predict\n# newdata is the data on which the prediction is to be made\n\n# Visualising the Training set result\n# -----------------------------------\n\n# Using the ggplot2 library\nlibrary(ggplot2)\n\n# Plot of training set.\nggplot() +\n  geom_point(aes(x = training_set$YearsExperience, y = training_set$Salary),\n            color = 'red') +\n  geom_line(aes(x = training_set$YearsExperience, y = predict(regressor,newdata = training_set)),\n            color = 'blue') + \n  ggtitle('Salary vs Experience (Training set)') +\n  xlab('Years of experience') +\n  ylab('Salary')\n  \n# Plot of test set.\nggplot() +\n  geom_point(aes(x = test_set$YearsExperience, y = test_set$Salary),\n             color = 'red') +\n  geom_line(aes(x = training_set$YearsExperience, y = predict(regressor,newdata = training_set)),\n            color = 'blue') + \n  ggtitle('Salary vs Experience (Training set)') +\n  xlab('Years of experience') +\n  ylab('Salary')\n\n###\nrm(Salary_d,test_set,training_set,regressor,split,y_pred)\n\n#===========================================================================#\n\n\n# MULTIPLE LINEAR REGRESSION #\n#============================#\n\n# Importing Data\n#---------------\n\n# Setting up new working directory using the\nsetwd('D:\\\\Work\\\\ML\\\\Super Data Science\\\\Machine Learning A-Z Template Folder\\\\Part 2 - Regression')\n\n# Putting the data frame into an object called stats\ndataset <- read.csv('Section 5 - Multiple Linear Regression\\\\Multiple_Linear_Regression\\\\50_Startups.csv')\n\n# Exploring data\n# --------------\n\n# Summary of dataset\nsummary(dataset)\n# Structure of dataset\nstr(dataset)\n\n# Preprocessing\n# -------------\n\n# Factorizing the categorical variables\ndataset$State <- as.factor(dataset$State)\n# Encoding Categorical data\ndataset$State <- factor(dataset$State,levels = levels(dataset$State),labels = c(1:nlevels(dataset$State)))\n\n# Splitting the data set\n# ----------------------\n\n# Splitting the dataset into the training set and Test set\n# Using the package caTools\nlibrary(caTools)\nset.seed(123)\nsplit <- sample.split(dataset$Profit, SplitRatio = 2/3)\ntraining_set <- subset(dataset, split == TRUE)\ntest_set <- subset(dataset, split == FALSE)\n\n# Feature Scaling\n# ---------------\n\n# training_set[,] <- scale(training_set[,])\n# test_set[,] <- scale(test_set[,])\n# As the library used for linear regression already has featuring scaling built in, it does not requir feature scaling\n\n# Multiple Linear Model\n# ---------------------\n\n# Fitting Multiple Linear Regression to the training set.\n# Using the lm() function\n# creating regressor variable to store the linearmodel\nregressor = lm(formula = Profit ~ ., data = training_set)\n# formula = Profit ~ . is same as formula = Profit ~ R.D.Spend + Administration + Marketing.Spend + State\n\n# Information of the regressor\n# Using summary function\nsummary(regressor)\n# The function has feature to avoid dummy variable trap\n\n# Predicting the Test set results\n# Using the predict() function\n# storing the prediction into a functon\ny_pred <- predict(regressor,newdata = test_set)\n# Requirs the liner model to use as a basis to predict\n# newdata is the data on which the prediction is to be made\n\n# Building the optimal model\n\n# Using Backward Elimination\n# Starting with all the independent variables\nregressor = lm(formula = Profit ~ R.D.Spend + Administration + Marketing.Spend + State, data = training_set)\n# Then remove the non-significant independent variable\n# Using the summary function to find the non-significant independent variable\nsummary(regressor)\n\n# Removing the insignificant indpendent variable\nregressor = lm(formula = Profit ~ R.D.Spend + Marketing.Spend + State, data = training_set)\nsummary(regressor)\n\n# Removing the insignificant indpendent variable\nregressor = lm(formula = Profit ~ R.D.Spend + Marketing.Spend, data = training_set)\nsummary(regressor)\n\n# Removing the insignificant indpendent variable\nregressor = lm(formula = Profit ~ R.D.Spend, data = training_set)\nsummary(regressor)\n\n\n\n\n###\nrm(regressor,dataset,test_set,training_set,split,y_pred)\n\n#===============================================================================#\n\n# POLYNOMIAL LINEAR REGRESSION #\n#==============================#\n\n# Importing Data\n#---------------\n\n# Setting up new working directory using the\nsetwd('D:\\\\Work\\\\ML\\\\Super Data Science\\\\Machine Learning A-Z Template Folder\\\\Part 2 - Regression')\n\n# Putting the data frame into an object called stats\ndataset <- read.csv('Section 6 - Polynomial Regression\\\\Polynomial_Regression\\\\Position_Salaries.csv')\n\n# Exploring data\n# --------------\n\n# Summary of dataset\nsummary(dataset)\n# Structure of dataset\nstr(dataset)\n\n# Cleaning the dataset\n# Only extracting the independent and dependent variables\ndataset <- dataset[,2:3]\n\n\n\n# Splitting the data set\n# ----------------------\nif(FALSE){\n# As it is not required in this Perticular excercise\n\n# Splitting the dataset into the training set and Test set\n# Using the package caTools\nlibrary(caTools)\nset.seed(123)\nsplit <- sample.split(dataset$Profit, SplitRatio = 2/3)\ntraining_set <- subset(dataset, split == TRUE)\ntest_set <- subset(dataset, split == FALSE)\n}\n# Feature Scaling\n# ---------------\n\n# training_set[,] <- scale(training_set[,])\n# test_set[,] <- scale(test_set[,])\n# As the library used for linear regression already has featuring scaling built in, it does not requir feature scaling\n\n# Simple Linear Regression Model\n# ------------------------------\n\n# Fitting Simple Linear Regression to the training set.\n# Using the lm() function\n# creating regressor variable to store the linearmodel\nlin_reg = lm(formula = Salary ~ ., data = dataset)\n# The formula is dependent variable expressed as a linear combination of the independent variable\n# The data on which the to train needs to be specified.\n# Info about the regressor using summary() function\nsummary(lin_reg)\n\n# Predicting the Test set results\n# Using the predict() function\n# storing the prediction into a functon\ny_pred <- predict(lin_reg,newdata = dataset)\n# Requirs the liner model to use as a basis to predict\n# newdata is the data on which the prediction is to be made\n\n# Polynomial Linear Regression Model\n# ----------------------------------\n\n# Polynomial features\n# Adding polynomial features\ndataset$Level2 = dataset$Level^2\ndataset$Level3 = dataset$Level^3\ndataset$Level4 = dataset$Level^4\n\n# Fitting Polynomial Linear Regression to the training set.\n# Using the lm() function\n# creating regressor variable to store the linearmodel\npoly_reg <- lm(formula = Salary ~., data = dataset)\n# Info about the regressor using summary() function\nsummary(poly_reg)\n\n# Visualising the dataset result\n# ------------------------------\n\n# Using the ggplot2 library\nlibrary(ggplot2)\n\n# Plot of Simple Linear Regression\nggplot() +\n  geom_point(aes(x = dataset$Level, y = dataset$Salary),\n             color = 'red') +\n  geom_line(aes(x = dataset$Level, y = predict(lin_reg,newdata = dataset)),\n            color = 'blue') + \n  ggtitle('Salary vs Level (Simple Linear Regression)') +\n  xlab('Level') +\n  ylab('Salary')\n\n# Plot of Polynomial Regression\nggplot() +\n  geom_point(aes(x = dataset$Level, y = dataset$Salary),\n             color = 'red') +\n  geom_line(aes(x = dataset$Level, y = predict(poly_reg,newdata = dataset)),\n            color = 'green') + \n  ggtitle('Salary vs Level (Polynomial Regression)') +\n  xlab('Level') +\n  ylab('Salary')\n\n# Plot of both Regression\nggplot() +\n  geom_point(aes(x = dataset$Level, y = dataset$Salary),\n             color = 'red') +\n  geom_line(aes(x = dataset$Level, y = predict(lin_reg,newdata = dataset)),\n            color = 'blue') + \n  geom_line(aes(x = dataset$Level, y = predict(poly_reg,newdata = dataset)),\n            color = 'green') + \n  ggtitle('Salary vs Level') +\n  xlab('Level') +\n  ylab('Salary')\n\n# High Resolution\n# Making a grid of variables\n# Using the seq() Function to genreate an array of number that will increment\nx_grid = seq(min(dataset$Level),max(dataset$Level),0.1)\nAa=data.frame(Level=x_grid,Level2 = x_grid^2,Level3=x_grid^3, Level4=x_grid^4)\nggplot() +\n  geom_point(aes(x = dataset$Level, y = dataset$Salary),\n            color = 'red') +\n  geom_line(aes(x = x_grid$Level, y = predict(lin_reg,Aa)),\n            color = 'blue') + \n  geom_line(aes(x = x_grid                                                 \n                $Level, y = predict(poly_reg,Aa)),\n            color = 'green') + \n  ggtitle('Salary vs Level') +\n  xlab('Level') +\n  ylab('Salary')\n\n\n\n\n# Prediction\n# ----------\n# Let Aa be the new result\nAa = 6.5\n\n# Predicting a new result with linear Regression\ny_pred = predict(lin_reg,data.frame(Level=Aa))\n# Inputing the new value as dataframe\n\n# Predicting a new result with Polynomial Regression\ny_pred = predict(poly_reg,data.frame(Level=Aa,Level2 = Aa^2,Level3=Aa^3, Level4=Aa^4))\n\n###\nrm(dataset,Aa,lin_reg,poly_reg,y_pred,x_grid)\n#============================================================================#\n\n# SUPPORT VECTOR REGRESSION #\n#===========================#\n\n# Importing Data\n#---------------\n\n# Setting up new working directory using the\nsetwd('D:\\\\Work\\\\ML\\\\Super Data Science\\\\Machine Learning A-Z Template Folder\\\\Part 2 - Regression')\n\n# Putting the data frame into an object called stats\ndataset <- read.csv('Section 7 - Support Vector Regression (SVR)\\\\SVR\\\\Position_Salaries.csv')\n\n# Exploring data\n# --------------\n\n# Summary of dataset\nsummary(dataset)\n# Structure of dataset\nstr(dataset)\n\n# Cleaning the dataset\n# Only extracting the independent and dependent variables\ndataset <- dataset[,2:3]\n\n\n\n# Splitting the data set\n# ----------------------\nif(FALSE){\n  # As it is not required in this Perticular excercise\n  \n  # Splitting the dataset into the training set and Test set\n  # Using the package caTools\n  library(caTools)\n  set.seed(123)\n  split <- sample.split(dataset$Profit, SplitRatio = 2/3)\n  training_set <- subset(dataset, split == TRUE)\n  test_set <- subset(dataset, split == FALSE)\n}\n# Feature Scaling\n# ---------------\n\n# dataset[,] <- scale(dataset[,])\n# As the library used for linear regression already has featuring scaling built in, it does not requir feature scaling\n\n# Refression model\n# ----------------\n\n# Using the e1071 package\nlibrary(e1071)\n\n# Fitting Support Vector Regression to the dataset\n# Using the svm() function under the e1071 library\n# creating regressor variable to store the svr\nregressor <- svm(formula = Salary ~., data = dataset, type = 'eps-regression')\n# The formula is dependent variable expressed as a linear combination of the independent variable\n# The data on which the to train needs to be specified.\n# The type is the the most important argument, specifies the type of model\n# The type is set to eps as it is being used for regression\n\n# Info about the regressor using summary() function\nsummary(regressor)\n\n# Prediction\n# ----------\n# Let Aa be the new result\nAa = 6.5\n\n# Predicting a new result with linear Regression\ny_pred = predict(regressor,data.frame(Level=Aa))\n\n# Visualising the dataset result\n# ------------------------------\n\n# Using the ggplot2 library\nlibrary(ggplot2)\n\n# Plot of SVR Results\nggplot() +\n  geom_point(aes(x = dataset$Level, y = dataset$Salary),\n             color = 'red') +\n  geom_line(aes(x = dataset$Level, y = predict(regressor,newdata = dataset)),\n            color = 'blue') + \n  ggtitle('Salary vs Level (SVR Results)') +\n  xlab('Level') +\n  ylab('Salary')\n\n###\nrm(dataset, Aa,regressor,y_pred)\n#============================================================================#\n\n# DECISION REGRESSION #\n#=====================#\n\n# Importing Data\n#---------------\n\n# Setting up new working directory using the\nsetwd('D:\\\\Work\\\\ML\\\\Super Data Science\\\\Machine Learning A-Z Template Folder\\\\Part 2 - Regression')\n\n# Putting the data frame into an object called stats\ndataset <- read.csv('Section 8 - Decision Tree Regression\\\\Decision_Tree_Regression\\\\Position_Salaries.csv')\n\n# Exploring data\n# --------------\n\n# Summary of dataset\nsummary(dataset)\n# Structure of dataset\nstr(dataset)\n\n# Cleaning the dataset\n# Only extracting the independent and dependent variables\ndataset <- dataset[,2:3]\n\n# Splitting the data set\n# ----------------------\nif(FALSE){\n  # As it is not required in this Perticular excercise\n  \n  # Splitting the dataset into the training set and Test set\n  # Using the package caTools\n  library(caTools)\n  set.seed(123)\n  split <- sample.split(dataset$Profit, SplitRatio = 2/3)\n  training_set <- subset(dataset, split == TRUE)\n  test_set <- subset(dataset, split == FALSE)\n}\n# Feature Scaling\n# ---------------\n\n# dataset[,] <- scale(dataset[,])\n# Decision tree does not require any Fearture Scaling\n# As the library used for regression already has featuring scaling built in, it does not requir feature scaling\n\n# Refression model\n# ----------------\n\n# Using the rpart package\nlibrary(rpart)\n\n# Fitting Decision Regression to the dataset\n# Using the rpart() function under the rpart library\n# creating regressor variable to store the svr\nregressor <- rpart(formula = Salary ~., data = dataset, control = rpart.control(minsplit = 1))\n# The formula is dependent variable expressed as a linear combination of the independent variable\n# The data on which the to train needs to be specified.\n# The control argument is required to set the setting, such as number of split\n# 'control' takes in 'rpart.control()' function\n# within 'rpart.control()' function minsplit is set to 1\n\n# Info about the regressor using summary() function\nsummary(regressor)\n\n# Prediction\n# ----------\n# Let Aa be the new result\nAa = 6.5\n\n# Predicting a new result with linear Regression\ny_pred = predict(regressor,data.frame(Level=Aa))\n\n# Visualising the dataset result\n# ------------------------------\n\n# Using the ggplot2 library\nlibrary(ggplot2)\n\n# The visualisation requires high resolution\n# High Resolution\n# Making a grid of variables\n# Using the seq() Function to genreate an array of number that will increment\nx_grid = seq(min(dataset$Level),max(dataset$Level),0.001)\nAa=data.frame(Level=x_grid)\n\n# Plot of Decision Tree Results\nggplot() +\n  geom_point(aes(x = dataset$Level, y = dataset$Salary),\n             color = 'red') +\n  geom_line(aes(x = Aa$Level, y = predict(regressor,Aa)),\n            color = 'blue') + \n  ggtitle('Salary vs Level (Decision Tree Regression)') +\n  xlab('Level') +\n  ylab('Salary')\n\n\n###\nrm(dataset, Aa,regressor,y_pred,x_grid)\n#============================================================================#\n\n# RANDOM FOREST REGRESSION #\n#==========================#\n\n# Importing Data\n#---------------\n\n# Setting up new working directory using the\nsetwd('D:\\\\Work\\\\ML\\\\Super Data Science\\\\Machine Learning A-Z Template Folder\\\\Part 2 - Regression')\n\n# Putting the data frame into an object called stats\ndataset <- read.csv('Section 9 - Random Forest Regression\\\\Random_Forest_Regression\\\\Position_Salaries.csv')\n\n# Exploring data\n# --------------\n\n# Summary of dataset\nsummary(dataset)\n# Structure of dataset\nstr(dataset)\n\n# Cleaning the dataset\n# --------------------\n\n# Only extracting the independent and dependent variables\ndataset <- dataset[,2:3]\n\n# Splitting the data set\n# ----------------------\nif(FALSE){\n  # As it is not required in this Perticular excercise\n  \n  # Splitting the dataset into the training set and Test set\n  # Using the package caTools\n  library(caTools)\n  set.seed(123)\n  split <- sample.split(dataset$Profit, SplitRatio = 2/3)\n  training_set <- subset(dataset, split == TRUE)\n  test_set <- subset(dataset, split == FALSE)\n}\n# Feature Scaling\n# ---------------\n\n# dataset[,] <- scale(dataset[,])\n# Decision tree does not require any Fearture Scaling\n# As the library used for regression already has featuring scaling built in, it does not requir feature scaling\n\n# Refression model\n# ----------------\n\n# Using the rpart package\nlibrary(randomForest)\n\n# Setting seed\nset.seed(1234)\n\n# Fitting Random Forest Regression to the dataset\n# Using the randomForest() function under the randomForest library\n# creating regressor variable to store the svr\nregressor <- randomForest(x = dataset[1], y = dataset$Salary, ntree = 500)\n# The x argument is the independent variable and is taken in the form of a data frame.\n# The y argument is the dependent variable and is taken in the form of vector\n# The Ntrees is the number of trees to be made\n\n# Info about the regressor using summary() function\nsummary(regressor)\n\n# Prediction\n# ----------\n# Let Aa be the new result\nAa = 6.5\n\n# Predicting a new result with linear Regression\ny_pred = predict(regressor,data.frame(Level=Aa))\n\n# Visualising the dataset result\n# ------------------------------\n\n# Using the ggplot2 library\nlibrary(ggplot2)\n\n# The visualisation requires high resolution\n# High Resolution\n# Making a grid of variables\n# Using the seq() Function to genreate an array of number that will increment\nx_grid = seq(min(dataset$Level),max(dataset$Level),0.001)\nAa=data.frame(Level=x_grid)\n\n# Plot of Decision Tree Results\nggplot() +\n  geom_point(aes(x = dataset$Level, y = dataset$Salary),\n             color = 'red') +\n  geom_line(aes(x = Aa$Level, y = predict(regressor,Aa)),\n            color = 'blue') + \n  ggtitle('Salary vs Level (Random Forest Regression)') +\n  xlab('Level') +\n  ylab('Salary')\n\n\n###\nrm(dataset, Aa,regressor,y_pred,x_grid)\n\n#=============================================================================#\n\n",
    "created" : 1513883027231.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "3413439020",
    "id" : "A4EA0647",
    "lastKnownWriteTime" : 1514394125,
    "last_content_update" : 1514394125,
    "path" : "D:/Work/R/ML/Supervised learning Machine learning/Regression.R",
    "project_path" : null,
    "properties" : {
    },
    "relative_order" : 1,
    "source_on_save" : false,
    "source_window" : "",
    "type" : "r_source"
}